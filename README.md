# statistics-vs-ai


ğŸ“˜ Logistisk regresjon og nevrale nett

Dette prosjektet forklarer matematikken bak logistisk regresjon og korleis eit eitt-lags nevralt nett i realiteten er den same modellen. MÃ¥let er Ã¥ gjere det enkelt Ã¥ forstÃ¥ bÃ¥de samband og skilnader.


ğŸ” Oversikt
Kva er logistisk regresjon?
Kva gjer sigmoid-funksjonen?
Kva er log-loss?
Kvifor er eitt-lags nevrale nett eigentleg logistisk regresjon?
Kvifor varierer vektene mellom modellane?
Kodeeksempel i bÃ¥de scikit-learn og keras


ğŸ§  Logistisk regresjon â€“ grunnidÃ©
Logistisk regresjon modellerer sannsynet for at ei observasjon hÃ¸yrer til ein klasse:

p = Ïƒ(wáµ€x + b)

Der:
x = inputvariablar
w = vekter
b = bias
Ïƒ = sigmoid-funksjonen

Dette gir ein sannsynsverdi mellom 0 og 1.

ğŸ”¢ Sigmoid-funksjonen
Sigmoid gjer ein lineÃ¦r kombinasjon om til eit sannsyn:
Ïƒ(z) = 1 / (1 + e^(â€“z))

Eigenskapar:
Verdien er alltid mellom 0 og 1

Brukt direkte i logistisk regresjon
Brukt som aktivering i enkle nevrale nett

ğŸ“‰ Tapfunksjon: Log-loss
For Ã¥ trene modellen brukar vi log-loss:

L = -\[y \* log(p) + (1 âˆ’ y) \* log(1 âˆ’ p)]

Dette straffar:
hÃ¸ge sannsyn nÃ¥r modellen tek feil
lÃ¥ge sannsyn nÃ¥r modellen har rett

ğŸ¤– Ã‰in-lags nevralt nett = logistisk regresjon
Eit nevralt nett med berre eitt Dense-lag og ein sigmoid i utgangen:

p = Ïƒ(Wáµ€x + b)
Dette er identisk matematisk formel som i logistisk regresjon.
Forskjellen ligg i optimaliseringsmetoden, ikkje i modellen.

âš™ï¸ Optimalisering: Keras vs. scikit-learn

Modell	Optimisator	Type
scikit-learn LogisticRegression (lbfgs)	LBFGS	2.-ordens metode

Keras Dense + sigmoid	Adam / SGD	1.-ordens gradientmetodar

Poenget:
Modellane har same form, men treningsmetoden gjer at dei kan ende opp med ulike vekter.

â“ Kvifor fÃ¥r LR og NN ulike vekter?
SjÃ¸lv om modellane er matematiske tvillingar, kan vektene variere fordi:
Dei startar med ulike initialverdiar
Dei brukar ulike optimisarar
Dei har ulik lÃ¦ringsrate

Adam/SGD kan stoppe i andre minimum enn LBFGS

Men:
ğŸ‘‰ Beslutningsgrensa og prediksjonane blir svÃ¦rt like.
