# statistics-vs-ai





ğŸ“˜ Logistisk regresjon og nevrale nett



Ei intuitiv forklaring pÃ¥ nynorsk



Dette prosjektet forklarar matematikken bak logistisk regresjon og korleis eit eitt-lags nevralt nett i realiteten er den same modellen. MÃ¥let er Ã¥ gjere det enkelt Ã¥ forstÃ¥ bÃ¥de samband og skilnader.



ğŸ” Oversikt



Kva er logistisk regresjon?



Kva gjer sigmoid-funksjonen?



Kva er log-loss?



Kvifor er eitt-lags nevrale nett eigentleg logistisk regresjon?



Kvifor varierer vektene mellom modellane?



Kodeeksempel i bÃ¥de scikit-learn og keras



Presentasjon i PowerPoint-format



ğŸ§  Logistisk regresjon â€“ grunnidÃ©



Logistisk regresjon modellerer sannsynet for at ei observasjon hÃ¸yrer til ein klasse:



p = Ïƒ(wáµ€x + b)





Der:



x = inputvariablar



w = vekter



b = bias



Ïƒ = sigmoid-funksjonen



Dette gir ein sannsynsverdi mellom 0 og 1.



ğŸ”¢ Sigmoid-funksjonen



Sigmoid gjer ein lineÃ¦r kombinasjon om til eit sannsyn:



Ïƒ(z) = 1 / (1 + e^(â€“z))





Eigenskapar:



Verdien er alltid mellom 0 og 1



Brukt direkte i logistisk regresjon



Brukt som aktivering i enkle nevrale nett



ğŸ“‰ Tapfunksjon: Log-loss



For Ã¥ trene modellen brukar vi log-loss:



L = -\[y \* log(p) + (1 âˆ’ y) \* log(1 âˆ’ p)]





Dette straffar:



hÃ¸ge sannsyn nÃ¥r modellen tek feil



lÃ¥ge sannsyn nÃ¥r modellen har rett



ğŸ¤– Ã‰in-lags nevralt nett = logistisk regresjon



Eit nevralt nett med berre eitt Dense-lag og ein sigmoid i utgangen:



p = Ïƒ(Wáµ€x + b)





Dette er identisk matematisk formel som i logistisk regresjon.



Forskjellen ligg i optimaliseringsmetoden, ikkje i modellen.



âš™ï¸ Optimalisering: Keras vs. scikit-learn

Modell	Optimisator	Type

scikit-learn LogisticRegression (lbfgs)	LBFGS	2.-ordens metode

Keras Dense + sigmoid	Adam / SGD	1.-ordens gradientmetodar



Poenget:

Modellane har same form, men treningsmetoden gjer at dei kan ende opp med ulike vekter.



â“ Kvifor fÃ¥r LR og NN ulike vekter?



SjÃ¸lv om modellane er matematiske tvillingar, kan vektene variere fordi:



Dei startar med ulike initialverdiar



Dei brukar ulike optimisarar



Dei har ulik lÃ¦ringsrate



Adam/SGD kan stoppe i andre minimum enn LBFGS



Men:



ğŸ‘‰ Beslutningsgrensa og prediksjonane blir svÃ¦rt like.



ğŸ“ Eksempel: Python-kode

Logistic Regression (sklearn)

from sklearn.linear\_model import LogisticRegression



lr = LogisticRegression(penalty=None)

lr.fit(X\_train\_scaled, y\_train)



print("Test accuracy:", lr.score(X\_test\_scaled, y\_test))



Ã‰in-lags nevralt nett (Keras)

import tensorflow as tf



nn = tf.keras.Sequential(\[

&nbsp;   tf.keras.layers.Dense(1, activation='sigmoid')

])



nn.compile(optimizer=tf.keras.optimizers.Adam(0.001),

&nbsp;          loss='binary\_crossentropy',

&nbsp;          metrics=\['accuracy'])



nn.fit(X\_train\_scaled, y\_train, epochs=500)



ğŸ¨ Presentasjon

